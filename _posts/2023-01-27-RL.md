---
layout: post
title: "강화학습 정리"
date: 2023-01-14 01:00:00 +0900
description: 강화학습의 수학적 기반
tags: deep-learning reinforcement-learning
giscus_comments: true
---

# 들어가는 글

보통 머신러닝을 처음 배울 때 머신러닝의 종류는 세 가지가 있다고 배운다. 지도학습, 비지도학습, 그리고 강화학습이다. 그런데 많은 머신러닝/딥러닝 입문 수업에서 지도학습과 비지도학습은 어느 정도 다루지만 강화학습은 다루지 않는 경우도 많은 것 같다. 이 때문에 강화학습을 접할 기회가 많지 않은 것 같아 이 포스팅에서 강화학습을 한번 살펴보겠다. 먼저 강화학습의 정의를 살펴보자.

> *Reinforcement Learning(강화학습)*: 어떤 환경 안에서 정의된 에이전트가 현재의 상태를 인식하여, 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법 (위키백과)

간단한 예제로 한번 살펴보자면, Pong(탁구) 게임을 플레이하는 에이전트를 생각해볼 수 있을 것이다. 

<p align="center" style="color:gray">
<img src="https://i.guim.co.uk/img/static/sys-images/Technology/Pix/pictures/2008/04/16/Pong460x276.jpg?width=465&quality=85&dpr=1&s=none"/><br>
Pong
</p>

이 경우 각각은 다음과 같다.

* input: 게임의 상태. 픽셀값
* output: 플레이어가 해야 하는 최적의 행동. 입력이 주어질 때 이것을 학습해야 한다.
* 환경(environment): 게임 프로그램
* 에이전트: 게임을 하는 플레이어. 에이전트가 바를 어떻게 움직이냐에 따라서 환경은 이에 반응해서 점수가 나오게 될 것이다.
* 보상(reward): 점수의 변화량

지도학습의 경우 offline에 input과 output이 주어지는 데 비해서, 비지도학습의 경우에는 output이 주어지지 않고 input의 구조를 파악하는 데에 집중한다. 반면 강화학습은 현재의 상태를 보고 어떤 행동을 해야할지 훈련하는 과정에서, 주변 환경과의 상호작용으로 시간이 지난 후 행동에 따른 보상이나 페널티가 어떻게 나왔는지를 기반으로 학습을 하게 된다. 이를 학습하여 에이전트는 어떤 행동을 해야 보상을 최대화할 수 있는지를 훈련받게 된다.

필자는 "정보과학세미나"라는 과목에서 강화학습을 한번 배운 적이 있지만, 당시 시간을 많이 투자하지 못하기도 했고 Tensorflow같은 프레임워크에 익숙하지 않은 상태였기 때문에 내용을 완전히 이해했다고 말하기는 힘든 상태였던 것 같다. 당시 배웠던 내용을 복습하며, 다른 책이나 자료들을 참고해 이 글로 정리해본다.

# Markov Reward Process

먼저, Markov Process라는 개념을 정의하자. 

> **Markov Process**(Markov Chain)\\
A Markov process is a pair $$(S, P)$$ where
* $$\mathcal{S}$$: a finite set of states
* $$\mathcal{P}$$: $$S^2 \to [0, 1]$$: a state transition probability

즉, 마르코프 과정(또는 체인)은 유한한 상태들의 집합과, 각 상태에서 다음 상태로 갈 확률을 명시해놓은 개념이다. 

<p align="center" style="color:gray">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Markovkate_01.svg/1920px-Markovkate_01.svg.png" width="40%"/><br>
Markov Process의 예시
</p>

위의 경우 첫 번째 행/열을 A, 두 번째 행/열을 E라고 한다면 transition probability  $$P$$는 다음과 같이 행렬로 쓸 수 있다.

$$ \mathcal{P} = \begin{pmatrix}
0.6 & 0.4\\
0.3 & 0.7
\end{pmatrix}$$

Markov Process에서, 다음의 상태로 전이할 확률은 현재의 상태에만 의존하고, 그 전까지 상태들을 지나온 내역(history)에는 의존하지 않는다는 특징이 있다. 이를 식으로 표현하자면 다음과 같다. ($$S_t$$: 시간 $$t$$에서의 state)

$$
\mathbb{P}[S_{t+1}=s'|S_t=s, S_{t-1}, \cdots, S_1] = \mathbb{P}[S_{t+1}=s'|S_t=s] = \mathcal{P}(s, s')
$$

이 Markov Process에서 **reward function**(보상 함수)과 discount factor를 추가하면 **Markov Reward Process**가 된다.

> **Markov Reward Process**\\
A Markov reward process is a tuple $$(\mathcal{S}, \mathcal{P}, R, \gamma)$$ where
* $$(\mathcal{S}, \mathcal{P})$$: a Markov process
* $$R: \mathcal{S}\to \mathbb{R}$$: a reward function
* $$\gamma$$: a discount factor

여기에서 $$R$$은 현재의 state에 대한 보상이라고 생각할 수 있다. 어떤 state에 도달하기만 해도, $$R$$이라는 함수가 이에 대한 보상을 부여한다고 생각을 하자. 이것을 토대로 **State-Value Function**이라는 것을 정의한다. 이름에서 알 수 있듯이, 각 state에 대응하는 _가치(value)_를 부여하는 함수이다.

> Given an MRP $$(\mathcal{S}, \mathcal{P}, R, \gamma)$$, its **state-value function** $$v: \mathcal{S} \to \mathbb{R} $$ is
* $$v(s) = \mathbb{E}[\sum_{k=0}^\infty \gamma^k R(N_k(s))]$$ \\
where $$N_k(s)$$ is the random variable describing the state after $k$ steps from $$s$$.

state-value function은 미래에 받을 보상까지도 모두 기댓값으로 계산해 고려한 것이라고 할 수 있다. 물론 미래에 받을 보상을 현재에 받을 보상과 똑같이 취급하지는 않고, $$k$$번의 스텝 후에 받을 보상은 $$\gamma^k$$만큼 절하해서 고려하는 것이다. $$\gamma$$가 낮을수록 현재의 보상에 더 충실해질 것이고, 높을수록 현재를 조금 희생하더라도 미래에 큰 보상을 받는 방향을 추구하게 될 것이다.

그런데 $$v$$는 계산 과정에서 무한급수가 등장하므로 사용이 불편하다. 식을 정리해서 무한급수가 등장하지 않도록 해보자.

$$ v(s) = \mathbb{E} \left[\sum_{k=0}^\infty \gamma^k R(N_k(s)) \right]$$

$$= \mathbb{E}\left[
    R(s) + \gamma \sum_{k=0}^\infty \gamma^k R(N_k(N(s)))
\right]$$

$$ = \mathbb{E}\left[R(s)  + \gamma \cdot v(N(s)) \right] $$

$$ = R(s) + \gamma \cdot \sum_{s'\in \mathcal{S}} \left(\mathcal{P}(s, s') v(s')\right)$$

이를 행렬로 나타내면 다음과 같다. 

$$\left(\begin{array}{c}v\left(s_1\right) \\ \vdots \\ v\left(s_n\right)\end{array}\right)=\left(\begin{array}{c}R\left(s_1\right) \\ \vdots \\ R\left(s_n\right)\end{array}\right)+\gamma\left(\begin{array}{ccc}\mathcal{P}\left(s_1, s_1\right) & \ldots & \mathcal{P}\left(s_1, s_n\right) \\ \vdots & \ddots & \vdots \\ \mathcal{P}\left(s_n, s_1\right) & \ldots & \mathcal{P}\left(s_n, s_n\right)\end{array}\right)\left(\begin{array}{c}v\left(s_1\right) \\ \vdots \\ v\left(s_n\right)\end{array}\right)$$

즉, 

$$ v= R + \gamma \mathcal{P} v$$

이므로

$$ v = (1-\gamma \mathcal{P})^{-1}R$$

이 된다.

# Markov Decision Process

이번에는 **Markov Decision Process**라는 것을 정의한다. 비슷한 용어가 계속 나오지만 헷갈리지 말자. 사실 이걸 정의하기 위해서 앞의 개념들이 나온 것이다.

> **Markov Decision Process**\\
A Markov decision process is a tuple $$(\mathcal{S}, \mathcal{A}, \mathcal{P}, R, \gamma)$$ where
* $$\mathcal{S}$$: a finite set of states
* $$\mathcal{A}$$: a finite set of **actions**
* $$\mathcal{P}: \mathcal{S\times A\times S}\to [0, 1]$$: a state transition probability
* $$R: \mathcal{S\times A}\to \mathbb{R}$$: a reward function 
* $$\gamma \in [0, 1]$$: a discount factor

정의를 보면 action이라는 개념이 새로 나온 것을 제외하면 앞의 Markov reward process와 크게 다르지 않다. 또, MRP에서는 현재의 state가 주어지면 다음 state가 무엇이 될지는 확률적으로 결정될 뿐이었는 데 비해서 MDP에서는 $$P$$가 action에도 의존하는 것이 차이점이다. 즉, **에이전트가 어떤 액션을 가해서 다음 state가 무엇이 될 지 어느 정도 영향을 미칠 수 있는 것이다**. Reward 또한 정의가 바뀌었다. 이제 $$R(s, a)$$는 다음과 같이 정의한다.

> $$R(s, a)$$: 현재 state $$s$$에서 $$a$$를 택했을 때, 다음 state들에서 받을 수 있는 expected reward

즉 보상 함수는 현재의 state에 따라서 부여되는 것이 아니라, 현재의 state에서 어떤 action을 취하냐에 따라서도 달라지게 된다는 것이 차이점이다.

**강화학습의 대상이 되는 문제들은 대부분 environment가 Markov decision process로 나타낼 수 있는 것들이다**.

이제 주어진 MDP $$(\mathcal{S, A, P}, R, \gamma)$$ 에 대해 **Policy**(정책)라는 것을 정의한다.

> **Policy**
A policy of an MDP $$(\mathcal{S, A, P}, R, \gamma)$$ is a probability distribution over actions given states: $$\pi: \mathcal{S \times A} \to [0, 1]$$

말이 어려워보이지만 간단히 말해서, 현재의 상태를 보고 에이전트가 어떤 액션을 취할지 결정하는 함수가 바로 policy이다. 이렇게 생각하면 policy(정책)이라는 이름이 잘 지어진 것이라고 느껴진다. 즉, 현재의 state에 따라서 어떤 action이 취해질지가 (확률적으로) 결정되고, 이렇게 결정된 action은 다시 현재의 state와 함께 다음의 state를 확률적으로 결정하는 데 사용된다.

MDP $$(\mathcal{S, A, P}, R, \gamma)$$와 Policy $$\pi$$가 주어져있다면 state끼리 이동하는 과정은 그냥 Markov Process $$(\mathcal{S}, \mathcal{P}^\pi)$$ 가 된다. 당연한 것이, 현재의 state를 보고 어떤 action을 취할지가 $$\pi$$에 다 명시되어 있고, 이 action에 따라 다음 state가 뭐가 될 지는 $$\mathcal{P}$$에 나와있기 떄문이다. 이때 현재 state $$s$$에서 다음 state $$s'$$으로 transition할 확률

$$ \mathcal{P}^\pi(s, s') = \sum_{a\in \mathcal{A}} \pi(s, a) \cdot \mathcal{P} (s, a, s')$$

이다. 조금만 생각해보면 당연하게 이해할 수 있다. 또한, 

$$ \mathcal{R}^\pi(s) = \sum_{a\in \mathcal{A}} \pi(s, a) \cdot R(s, a) $$

로 정의하면 $$(\mathcal{S, P^\pi, R^\pi}, \gamma)$$는 Markov reward process이다.

강화학습의 목표는 이 policy를 최적화하는 것이 된다. 즉 현재의 상태(state)를 보고 에이전트가 어떤 액션을 취할지를 최적화하여 보상을 최대화하는 것이 목표이다.

한편, 아까 MRP를 정의하면서 각 state에 가치를 부여하는 state-value function이라는 것이 있었던 것을 기억할 것이다. 그런데 이제 MDP에서는 다음의 state는 현재의 state에만 의존해서 결정되는 것이 아니다. action을 어떻게 취할지에 따라서 다음의 state가 달라지고, 또 에이전트가 받게 될 보상이 달라지는 것이다. 따라서 MDP에서는 우리가 취해줄 action 하나하나에 가치를 매겨주는 함수가 필요하다. 이름은 **action-value function**이라고 하면 알맞을 것이다.

> **Action-Value Function**\\
The action-value function $$q^\pi: \mathcal{S\times A}\to \mathbb{R}$$ is given by
* $$q^\pi(s, a) = R(s, a) + \gamma \sum_{s'\in \mathcal{S}}\left(\mathcal{P}(s, a, s') \cdot v^\pi (s'))\right)$$ where
* State-value function $$v^\pi(s) = \mathbb{E}\left[
    \sum_{k=0}^\infty \gamma^k R^\pi(N_k^\pi(s))
\right]$$

식이 어려워보이지만, 의미를 생각하면 크게 어렵지 않다. policy $$\pi$$가 이미 주어져 있으니, state가 주어지면 어떤 action을 취해야 할지는 확률분포가 바로 결정된다. 이 확률분포를 따라서 예상되는 reward의 기댓값을 구하고, $$\gamma$$로 discount해준 것일 뿐이다. 

이제 대부분의 강화학습 환경을 모델링할 수 있는 Markov decision process를 알게 되었고, 현재의 상태를 보고 어떤 액션을 취해야 할지를 policy 함수로 결정하면 된다는 것도 알게 되었다. 또, 이러한 policy가 주어졌을 때 각각의 상태와 액션을 어떻게 평가해야 할지 또한 state-value function, action-value function을 정의함으로써 해결하였다. 이제 강화학습의 궁극적 목표인 policy를 결정하는 것만 해내면 된다. 그렇다면 어떤 policy가 좋은 policy일까?

# Optimizing Policy




# 참고문헌
Lecture 06: Reinforcement Learning, CS Semiar (Machine Learning in Practice), Korea Science Academy of KAIST
