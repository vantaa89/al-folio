---
layout: post
title: Flow Model
date: 2022-12-16 19:29:00 +0900
description: Flow model에 대한 설명
tags: deep-learning MathDNN
giscus_comments: true
---

> Erneset Ryu 교수님의 2022학년도 2학기 \<심층신경망의 수학적 기초\> 과목을 듣고 필자가 요약해 정리한 글입니다.

**Flow model**은 GAN(Generative Adversarial Network)이나 VAE(Variational Autoencoder)와 같은 생성 모델의 하나이다. 두 네트워크가 서로 경쟁하며 생성을 한다는 식으로 비교적 쉽게 이해할 수 있는 GAN과, (적어도 구현할 때) 표면적으로는 어느 정도 와닿는 VAE와 달리 Flow model은 그 수학적 기반을 어느 정도 제대로 이해해야 구현을 할 수 있는 것 같다. 이 때문에 필자도 공부를 해보려다가도 엄두를 못 내고 있었는데, 이번 학기에 \<심층신경망의 수학적 기초\>를 들으면서 Flow model이 나와서 공부할 기회가 있었다. 수업을 들으면서 배운 내용을 간단하게 정리해보겠다.

# Flow Model이란

먼저 Flow model이 무엇인지, 어떤 방법을 사용하는지부터 간단하게 짚고 넘어가보자. 

Flow model은 **probabilistic generative model**의 하나이다. 주어진 데이터셋 $$ X_1, X_2, \cdots, X_N$$이 있다고 하자. 여기서, "나올 수 있는 데이터들의 집합"이라는 것이 있고, 더 나아가서 나올 수 있는 데이터들의 확률분포가 있다고 생각을 해보자. 즉, $$ X_1, X_2, \cdots, X_N$$는 $$p_{true}$$라는 확률분포로부터 샘플링을 한 것이 된다.

$$ X_1, X_2, \cdots, X_N \sim p_{true}$$

Flow model과 같은 probabilistic generative model의 목표는 이 확률분포 $$ p_{true}$$를 모사하는 것이다. 새로운 샘플 $$ X\sim p_{\theta}$$를 만들 수 있도록 하는 새로운 확률분포 $$p_{\theta}$$를 적당히 매개화하여 나타내는 것이 목표가 된다. 

이러한 공통의 목표를 GAN, VAE, 그리고 Flow model은 다음과 같이 다른 방식으로 수행한다.
<p align="center" style="color:gray">
<img src="https://lilianweng.github.io/posts/2018-10-13-flow-models/three-generative-models.png" width="80%">
<br>
GAN, VAE, 그리고 Flow Model의 비교 (출처: https://lilianweng.github.io/posts/2018-10-13-flow-models/)
</p>

공통적으로, 세 모델은 모두 입력 데이터 $$X$$ 를 **잠재공간(latent space)**상의 벡터 $$Z$$ 와 대응시키는 식으로 동작한다. 이떄 잠재공간이란 일종의 차원축소라고도 볼 수 있는데, VAE의 경우를 보면 decoder는 $$Z$$만을 보고도 원래의 $$X$$와 유사한 $$X'$$을 복원해낸다. 이 말은 $$Z$$에 $$X$$에 담겨있는 정보(feature)들이 대부분 담겨있다는 뜻으로도 생각할 수 있을 것이다. 

눈에 띄는 것은 VAE와 Flow model의 차이이다. VAE에서 encoder와 decoder가 각각 분포를 나타내었고, 두 네트워크가 별개의 가중치를 가졌던 것과 달리(<a href="/blog/2022/VAE/">VAE 포스팅 참고</a>) flow에서는

1. 입력 데이터를 latent space로 보내는, encoder역할을 하는 것이 조건부 확률분포가 아닌 함수 $$f$$ 이다. 즉, 하나의 $$X$$ 에 대해 $$z$$ 가 유일하게, deterministic하게 결정된다.

1. $$f$$는 가역(invertible)이다. 즉, latent vector로부터 데이터를 샘플링하기 위해 별개의 네트워크를 사용할 필요가 없다!

1. VAE에서는 log likelihood 대신 이를 근사하는 ELBO(또는 VLB)를 최대화하였다면, Flow model에서는 log likelihood 그 자체를 최대화한다.

그렇다면 몇 가지 의문점이 생긴다.
* 잠재벡터 $$Z$$는 왜 필요한 것일까?
* 기존의 컨볼루션, linear layer, ReLU등으로 구성된 신경망은 역변환이 불가능한데, $$f$$를 어떻게 invertible하게 만들 수 있을까? 
* 인코더와 디코더를 거친 후 reconstruction loss를 구하면 되는 Autoencoder와 달리, flow model에서는 $$f^{-1}\circ f(X)=X$$가 된다. 그러면 트레이닝은 어떻게 시켜야 할까?

Flow model이 등장한 모티베이션과 그 이론적 배경들을 살펴보면서, 위의 의문점들을 하나씩 해결해보자.

# Maximum Likelihood Estimation

앞서 언급했듯이, flow model을 훈련하는 목표는 **maximum likelihood estimation(MLE, 최우도추정)** 을 수행하는 것이다. 분포 $$p_\theta(x)$$에서 나온 표본 $$X_1, X_2, \cdots, X_n$$이 있을 때, 우도(가능도, likelihood)는 

$$ p_\theta(X_1) p_\theta(X_2)\cdots p_\theta(X_n)$$

으로 정의된다. 이를 최대화하는 분포 $$p_\theta$$를, 다시 말해 이 분포를 결정하는 매개변수 $$\theta$$를 찾는 것이 바로 최우도추정이다. 이 우도라는 것은 무슨 의미를 가지는 것일까? 앞서 말했듯이, 우리가 flow model을 훈련시키는 목적은 데이터 $$X_1, X_2, \cdots, X_n$$에 내재되어 있는 구조를 파악하는 것이다. 이는 $$p_{true}$$에서 나온 것이므로 이를 근사하는 $$p_{\theta}$$를 구하는 것이 우리의 목표이다. 

만약 $$p_{\theta}$$가 실제 분포 $$p_{true}$$와 너무 다르다면, $$p_{\theta}$$는 어떤 데이터 $$X_i$$에 대해서 "이런 데이터는 나올 수 없어"라면서 매우 작은 값을 부여할 수도 있을 것이다. 그러면 우도가 매우 작아지게 된다. 따라서 우도가 클수록, 우리의 모델 $$p_{\theta}$$가 실제 분포 $$p_{true}$$에 근접해지는 것이다. 

그렇다면 각각의 $$X_i$$에 대해서만 $$p_\theta$$값을 크게 하면 되지 않나 하는 생각을 할 수 있을 것이다. 하지만 $$p_{\theta}$$는 확률밀도함수이기 때문에, 

$$\int p_{\theta}(x) dx = 1$$

이라는 기본적인 성질을 만족해야 한다. 즉, $$p_{\theta}$$가 부여할 수 있는 값의 "총량"은 제한되어 있으니, 샘플링된 데이터를 보고 함수값을 잘 배분해서 $$p_{true}$$와 비슷해지도록 하는 것이다. 

한편, 우도 

$$ p_\theta(X_1) p_\theta(X_2)\cdots p_\theta(X_n)$$

는 곱 형태로 되어 있기 떄문에 최대화를 하기가 불편하다. 경사하강법 등을 사용해서 최대화를 수행하려면 식을 미분해야 하는데, 곱 형태로 된 식은 미분이 어렵기 떄문이다. 따라서 로그를 씌워서 **log-likelihood**

$$ \sum_{i=1}^N \log{p_{\theta}(X_i)}$$

를 최대화해주게 된다.

# \\(p_{\theta}\\)의 표현

그러면 이제 "데이터를 받았을 때, $$p_{\theta}$$에서 $$X_i$$가 나올 확률$$을 neural network를 사용해서 나타내주면 된다. 간단하게 생각하면 이런 방법이 있을 것이다.

> 데이터 X를 입력으로 넣으면 $$p_{\theta}(X)$$를 출력으로 내보내는 신경망

그런데 이 방법에는 문제점이 있다. 먼저 정규화(normalization)을 시켜줄 수가 없게 된다. 만약 $$p_{\theta}$$가 이산확률분포였다면, 마지막 계층에 softmax를 한번 씌워주기만 하면 전부 더해서 1이 되도록 강제할 수 있다. 하지만 연속확률분포에서 (1) X를 마구 넣어서 각 X에 대해 $$p_{\theta}(X)$$의 함수값을 구하고 (2) 구분구적법과 같은 방법을 사용해서 적분을 하고 (3) 각각의 함수값을 $$ \int p_{\theta}(x)dx$$로 나눠서 정규화하는 것은 굉장히 어려운 작업이다. 따라서 우리는 다른 방법을 써준다. 입력 데이터 $$X$$를 정규분포와 같이 간단한 분포 상의 한 점으로 대응시키는 것이다. 

> 데이터 X를 입력으로 넣으면, 이를 간단한 분포 $$p_Z$$상의 
>$$Z= f_{\theta}(X) \sim p_Z$$
>로 대응시키는 신경망

이렇게 하면, $$p_{\theta}(x)$$는 자연스럽게 $$p_{\theta}(x)=p_Z(z)= p_Z(f_{\theta}(x))$$로 구할 수가 있게 된다.

이는 change of variables formula

$$p_{\theta}(X) = \left\vert\frac{\partial z}{\partial x}\right\vert p_{Z}(f_{\theta}(x))$$

와 같이 계산된다. 여기서 $$\left\vert\frac{\partial z}{\partial x}\right\vert = \left\vert\frac{\partial f_{\theta}}{\partial x}(x)\right\vert$$ 는 Jacobian 행렬의 행렬식(determinant)의 절대값으로, 일종의 부피 변환비같은 역할을 한다.

이제 이것을 log likelihood 식에 대입하면 우리의 training objective는 다음과 같다.

$$\text{maximize}_{\theta \in \Theta} \sum_{i=1}^N \log{p_{\theta}(X_i)} = \text{maximize}_{\theta \in \Theta} \sum_{i=1}^N \log{p_{Z}(f_{\theta}(X_i))} + \log{\left\vert\frac{\partial f_\theta}{\partial x}(X_i)\right\vert} $$

SGD나 Adam같은 optimizer로 위의 최적화를 수행하기만 하면 flow를 훈련시킬 수 있게 되는 것이다. 그런데, 

$$\log{p_{Z}(f_{\theta}(X_i))} + \log{\left\vert\frac{\partial f_\theta}{\partial x}(X_i)\right\vert}$$

의 기울기를 구하고, 샘플링을 통해 새로운 데이터를 얻어내려면 몇 가지 요구사항이 필요하다.

* 먼저, $$f_{\theta}$$와 $$\nabla_{\theta}f_{\theta}$$를 쉽게 계산할 수 있어야 한다.
* 식에 $$\frac{\partial f_\theta}{\partial x}$$가 나오기 때문에 $$f_\theta$$는 ($$x$$에 대해)미분가능해야 할 것이다.
* 여기에 $$\nabla_{\theta}$$를 씌워서 값을 얻을 수 있어야 하므로,$$ \nabla_{\theta}\left\vert\frac{\partial f_\theta}{\partial x}\right\vert$$도 쉽게 계산이 가능해야 한다.
* 마지막으로, sampling을 하기 위해서는 $$f_{\theta}$$가 가역(invertible)이어야 한다.
    * 부연설명을 하자면, 여기서 sampling이란 우리가 근사한 분포 $$p_{\theta}$$로부터 새로운 데이터를 얻어내는 것을 말한다. 이는 $$Z\sim p_Z$$를 하나 샘플링 한 후 $$f^{-1}_\theta(Z)$$를 구함으로써 가능하다. $$f_{\theta}$$가 가역이어야 하는 이유이다.

## Flow의 합성
기존의 MLP(다층 퍼셉트론)나 CNN(컨볼루션 신경망)에서 층을 여러 개 쌓아서 네트워크의 표현력을 증가시켰듯이, flow에서도 비슷하게 flow를 여러 층으로 쌓아 합성함수를 구성하듯이 표현력을 증가시킬 수 있다. 단순히

$$x \mapsto f_1 \mapsto f_2 \mapsto \cdots \mapsto f_n \mapsto z$$

와 같이 여러 함수들을 거쳐서 $$x$$가 간단한 분포 $$p_Z$$를 따르는 확률변수로 변환되는 것이다. 이 경우 체인 룰을 사용해서 

$$\left\vert \frac{\partial f_{\theta}}{\partial x}\right\vert = \left\vert \frac{\partial f_n}{\partial f_{n-1}}\frac{\partial f_{n-1}}{\partial f_{n-2}}\cdots \frac{\partial f_2}{\partial f_1}\frac{\partial f_1}{\partial x}\right\vert$$

$$ = \left\vert \frac{\partial f_n}{\partial f_{n-1}}\right\vert\left\vert\frac{\partial f_{n-1}}{\partial f_{n-2}}\right\vert\cdots \left\vert\frac{\partial f_2}{\partial f_1}\right\vert\left\vert\frac{\partial f_1}{\partial x}\right\vert$$

임을 알 수 있으므로, 

$$ \log p_\theta(x) = \log p_Z(f_\theta(x))+\sum_{i=1}^n \log \left\vert \frac{\partial f_i}{\partial f_{i-1}}\right\vert$$

# Coupling Flow

이제 처음에 들었던 세 가지 의문점 중 첫 번째와 세 번째가 해결되었다. 그렇다면 마지막, 두 번째 의문점이 남는다.
* 기존의 컨볼루션, linear layer, ReLU등으로 구성된 신경망은 역변환이 불가능한데, $$f$$를 어떻게 invertible하게 만들 수 있을까?

일반적으로, 이를 위해서 사용하는 방법은 **coupling flow**이다. 이를 간단하게 설명하자면, 각 층의 입력 $$x$$를 $$x = (x^A\vert x^B)$$로 분할한 후 $$x^B$$만 변환하고 $$x^A$$는 그대로 남겨두는 것이다. 수식으로 이를 설명하자면 다음과 같다.

$$ z = (z^A\vert z^B) = f(x) = f(x^A\vert x^B) = (x^A\vert \hat{f}(x^B\vert \psi_\theta (x^A)))$$

식을 보면, $$x^B$$를 변환하는 함수는 $$\hat{f}(x^B\vert \psi_\theta (x^A))$$로 표현된다는 것을 알 수 있다. 즉, $$x^A$$로부터 $$ \psi_\theta (x^A)$$라는 값을 얻은 후, $$x^B$$를 변환할 때 이를 참고해서 사용한다는 것으로 해석하면 된다. 또, $$ \hat{f}$$는 가역이라는 조건이 붙는다.

이렇게 하면 $$z$$만 보고 $$x$$를 쉽게 얻어낼 수 있다! 먼저 $$z^A = x^A$$이므로 $$x^A$$는 바로 얻어낼 수 있다.  $$x^B$$만 얻어내면 되는 것인데, 이는 다음의 과정으로 계산이 가능하다.

1. 먼저 $$\psi(x^A)=\psi(z^A)$$를 구한다.
1. $$z^B = \hat{f}(x^B\vert\psi(x^A))$$이고, $$\hat{f}$$는 가역이며, $$\psi(x^A)$$를 알고 있으니 함수의 역을 취해서 $$x^B$$를 얻어낼 수 있다.

즉, $$f$$는 가역 함수가 된다. 가역함수끼리 합성해도 가역함수가 되므로, 이러한 $$f$$를 앞서 설명한 것처럼 거듭해 쌓으면 더 강력한 표현력을 가진 flow를 얻을 수 있다. 한편, coupling flow의 Jacobian determinant $$\left\vert\frac{\partial f_\theta}{\partial x}\right\vert$$를 구하면 다음과 같다.

$$\left\vert\frac{\partial f_\theta}{\partial x}\right\vert = \left\vert \frac{\partial \hat{f}}{\partial x^B}(x^B\vert \psi_{\theta}(x^A))\right\vert$$

## Additive Transformation (NICE)

L. Dinh et al. (2014), "Nice: Non-linear independend components estimation"에서 사용한 방법이다. 단순히

$$ z_{1:n/2} = x_{1:n/2}, z_{n/2:n} =x_{n/2:n} = x_{n/2:n }+t_\theta(x_{1:n/2})$$

로 정의하는 방식이다. 즉, 입력 데이터의 앞부분 절반은 변형 없이 그대로 내보내고, 나머지 절반은 앞부분으로부터 얻은 어떤 함수값을 더해주기만 하는 방식이다. 이 경우 inverse는

$$ x_{1:n/2} = z_{1:n/2}, x_{n/2:n} =z_{n/2:n} = x_{n/2:n }-t_\theta(z_{1:n/2})$$

로 쉽게 계산할 수 있게 되고, Jacobian determinant는 $$\left\vert\frac{\partial f_\theta}{\partial x}\right\vert = 1$$이 된다.

## Affine Transformation (Real NVP)

L. Dinh et al. (2016), "Density estimation using real nvp"에서 사용된 방법이다. Real NVP에서 NVP는 non-volume-preserving의 약자로, 일종의 부피변화율의 의미를 갖는다고 한 Jacobian determinant가 NICE에서와는 다르게 1이 아니게 된다는 점에서 따온 말인 것으로 보인다. (논문을 읽지는 않아서 정확히는 모르겠다)

Real NVP에서는 한발 더 나아가, 

$$ z_{1:n/2} = x_{1:n/2},\quad z_{n/2:n} =e^{s_\theta(x_{1:n/2})} \odot x_{n/2:n} +t_\theta(x_{1:n/2})$$

와 같이 계산이 이루어진다. 여기서 $$\odot$$은 elementwise multiplication을 의미한다. inverse는 (당연히) 

$$ x_{1:n/2}=z_{1:n/2}, \quad x_{n/2:n} = e^{-s_\theta(x_{1:n/2})}\odot (z_{n/2:n}-t_\theta(x_{1:n/2}))$$

로 계산할 수 있다. Jacobian determinant는 앞서 말했듯이 1이 아니게 되고, 게산을 해보면 

$$\left\vert\frac{\partial f_\theta}{\partial x}\right\vert = \prod_{i}e^{s_\theta(x_{1:n/2})_i}=\text{exp}(\mathbb{1}_{n/2}^T s_{\theta}(x_{n/2:n}))$$

이 된다.

## Coupling Layer의 분할 방법
위에서 $$x  = (x^{A}, x^B)$$와 같이 분할할 때, 분할하는 방법을 계속 똑같이 둔다면 $$x^A$$에 해당하는 원소(픽셀)들은 계속 값이 바뀌지 않은 채로 놓이게 될 것이다. 따라서 일반적으로 flow model들에서는 각 계층마다 분할을 하는 방법을 계속 바꿔준다.

<p style="color:gray" align="center">
<img src="/assets/img/realnvp_masks.png" width="80%">
<br>
coupling layer의 분할 방법. 좌: spatial checkerboard. 우: channelwise. 출처: L. Dinh et al. (2016)
</p>

첫 번째 방법은 위의 왼쪽 그림처럼 checkerboar pattern으로 A와 B를 구분해주는 방법이다. 반면 두 번째 방법은 텐서를 reshape해준 후, 채널별로 A와 B를 구분해주는 방법이다. Real NVP의 경우 두 가지 방법을 교대로 3번씩 사용해주는 방식으로 충분한 표현력을 얻을 수 있도록 한다.

# 결론
이제 글의 초반에 던졌던 세 가지 질문에 대한 답을 정리해보겠다.

* 잠재벡터 $$Z$$는 왜 필요한 것일까?
    * 각 데이터 $$X$$의 우도를 계산할 때, 뉴럴 네트워크가 곧바로 PDF를 출력하도록 하기에는 (1) normalization이 힘들고, (2) sampling이 어렵다는 문제점이 있다. flow에서는 이를 해결하기 위해서 주어진 데이터의 확률분포를 정규분포와 같은 간단한 분포로 매핑하는 함수 $$f_\theta$$를 정의하는데, 이것에 의해 입력 데이터 $$X_i$$는 잠재벡터 $$Z=f_\theta(X_i)$$로 매핑된다.
* 기존의 컨볼루션, linear layer, ReLU등으로 구성된 신경망은 역변환이 불가능한데, $$f$$를 어떻게 invertible하게 만들 수 있을까? 
    * Coupling Flow를 사용해서 텐서를 두 부분으로 분할한 후, 절반은 그대로 두고 나머지 절반은 "첫번째 절반과의 correlation이 들어간" 가역 변환을 취해준다. NICE에서는 Additive Transformation을, Real NVP에서는 Affine Transformation을 사용한다.
* 인코더와 디코더를 거친 후 reconstruction loss를 구하면 되는 Autoencoder와 달리, flow model에서는 $$f^{-1}\circ f(X)=X$$가 된다. 그러면 트레이닝은 어떻게 시켜야 할까?
    * Maximum likelihood estimation을 트레이닝의 목표로 삼아, log likelihood을 최대화시킴으로서 트레이닝이 가능하다.
    