---
layout: post
title: NeRF(Neural Radiation Fields) 논문 리뷰
date: 2022-12-21 01:00:00 +0900
description: "Nerf: Representing scenes as neural radiance fields for view synthesis."
tags: deep-learning paper-review
giscus_comments: true
---

>Mildenhall, Ben, et al. "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis." Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I. 2020.

컴퓨터비전에서 단지 핫한 주제를 넘어 하나의 분야가 되어가고 있다는 NeRF를 제안한 첫 논문으로, ECCV 2020에서 Oral 발표를 하고 Best Paper에 Honorable Mention된 논문이다. 사실 읽은지는 조금 됐는데 종강하고 이제야 정리해서 올린다.

# Abstract & Introduction
**NeRF**는 3차원 물체를 여러 각도(view)에서 찍은 사진들을 입력으로 주면, 새로운 각도에서 보았을 때 바라본 모습을 합성(view synthesis)하는 모델이다.

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/JuH79E8rdKc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br>
NeRF 프로젝트 공식 영상
</p>

**View Synthesis**를 위해서는 복잡한 기하학적 구조와 각 재질의 반사율(reflectance property) 등을 잘 고려해 처리해줘야 한다. 이 때문에, 현재까지 많은 method들이 제안되어왔지만 충분히 사실적인 품질의 이미지를 생성하는 모델은 존재하지 않았다.

NeRF는 이 문제를 뉴럴 네트워크를 사용해 해결하였다. 이때, 비전 문제에 쓰이는 CNN(컨볼루션 신경망)이 아닌, **fully connected network**를 사용한다는 점이 특이한 점이다. 이는 사진들의 집합으로부터 바로 새로운 각도에서의 사진을 합성해내는 것이 아니라, 입력으로 $$(x, y, z, \theta, \phi)$$ (위치 3차원 + 보이는 각도 2차원)를 받으면 그 위치에서의 **volume density**와 **view-dependent emitting radiance**(RGB 색상)를 출력하도록 모델이 훈련되기 때문이다. 5차원 입력을 받는 연속함수를 근사하는 것일 뿐이므로 CNN을 사용할 이유가 없다. 뉴럴넷을 통해 임의의 방향에서 어떤 바라보았을 때의 빛의 세기나 색을 모두 알 수 있다면 그 물체를 어떤 방향에서 보아도 어떻게 보이는지를 알 수 있을 것이다.

<p align="center" style="color:gray">
<img src="/assets/img/NeRF/fig1.jpeg" width="80%"/>
</p>

NeRF를 훈련시킨 후에 새로운 각도에서 보는 모습을 합성하는 과정은 다음과 같다. 
1. 3차원 공간 상의 점들을 샘플링한 후,
2. 각 점의 좌표와 그 점에서 물체를 바라보는 각도를 NeRF에 입력으로 넣어준 후
3. 출력으로 나온 색깔과 density를 사용해, classical한 렌더링 방법으로 2D 이미지를 구성해 낸다.

이때 volume rendering이라는 문제 자체가 특성상 미분가능할 수밖에 없기 떄문에, 경사하강법(gradient descent)을 사용해 입력 이미지와, NeRF가 렌더링한 이미지 사이의 차이를 최소화하는 방식으로 트레이닝을 하면 된다.

<p align="center" style="color:gray">
<img src="/assets/img/NeRF/fig2.jpeg" width="80%"/><br>
Overall pipeline
</p>

NeRF의 장점은 3차원 공간에 대한 정보를 voxel(픽셀의 3차원 버전)에 저장하는 것이 아니라 뉴럴넷의 가중치로써 저장하기 떄문에 저장공간을 획기적으로 줄일 수 있다는 것이다. 또한, NeRF는 기존의 SOTA(state-of-the-art) 모델과 비교하여 정성적/정량적으로 더 좋은 성능을 보이는 것으로 확인되었다.

# NeRF의 아키텍처
앞서 언급했듯이 NeRF는 5D vector-valued function을 MLP로 근사하여 사용한다. 이는

$$F_\Theta : (\mathbf{x, d}) \mapsto (\mathbf{c}, \sigma)$$

로 표현할 수 있다. 여기서 $$\mathbf{x}=(x, y, z)$$는 3차원 위치, $$d$$는 바라보는 각도 $$(\theta, \phi)$$를 3차원 단위벡터로 나타낸 것이며 출력 $$c$$는 색깔(RGB), $$\sigma$$는 volume density를 나타낸다. 

주의해야 할 것이, $$(x, y, z)$$는 보는 위치, 즉 관찰자의 위치가 아니라 **"관찰당하는 점"의 좌표**라는 것이다. 따라서 volume density $$\sigma$$는 해당 지점의 성질이므로 위치에만 의존하는 함수인 반면, $$c$$는 위치와 각도에 모두 의존하게 된다. 바라보는 각도에 따라서 반사율 등의 효과로 색깔이 달라질 수 있기 때문이다. 이를 고려하기 위해서 NeRF는 아래와 같은 아키텍처를 가진다.

<p align="center" style="color:gray">
<img src="/assets/img/NeRF/fig3.png" width="80%"/>
<br>
NeRF의 아키텍처. 파란색 계층들은 모두 256개 (또는 128개)의 뉴런으로 구성된 fully connected layer 다음에 ReLU 활성화 함수를 연결한 것이다. (<a href="https://towardsdatascience.com/nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis-ef1e8cebace4">출처</a>)
</p>

그림에서 볼 수 있듯이 fully connected layer를 8개 거친 후 NeRF는 $$\sigma$$와 256차원 벡터를 출력한다. $$\sigma$$는 $$\mathbf{d}$$ 가 입력으로 들어오기도 전에 출력되므로 각도에 의존하지 않는 상태이다. 256차원 벡터는 다시 각도를 나타내는 $$\mathbf{d}$$와 이어붙여져서(concatenate) 뉴런 128개짜리 fully connected layer를 한번 통과한 후에 RGB 색깔을 출력한다. 따라서 $$\mathbf{c}$$는 각도에 의존하게 된다. 

$$c$$의 경우, $$\mathbf{x}$$지점에서 $$\mathbf{d}$$ 방향으로 방사되는(radiate) 빛의 색과 세기이므로 **directional emitted radiance**라는 이름을 붙일 수 있을 것이다.

# Volume Rendering
이제 NeRF를 통해 공간 상의 임의의 지점에서의 volume density $$\sigma$$와 directional emitted radiance $$c$$를 알 수 있게 되었다. 사실 지금까지 volume density $$\sigma$$의 의미가 뭔지 모호했는데, 여기서 그 정의가 제대로 나오게 된다.

$$\sigma(\mathbf{x})ds$$는 광선이 $$\mathbf{x}$$를 포함하는 미소길이 $$ds$$만큼의 구간에서 막혀서 사라지는 확률이다. 정의에 의해서 카메라의 렌즈로 들어가는

$$\mathbf{r}(t) = \mathbf{o}+ t\mathbf{d} \quad\quad (t_n \le t \le t_f)$$

의 광선이 있을 때, 카메라가 이 광선으로부터 느끼는 빛의 세기와 색깔은

$$ C(\mathbf{r}) = \int_{t_n}^{t_f}T(t) \sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt\quad\text{where}\quad T(t) = \text{exp}\left(-\int_{t_n}^t \sigma(\mathbf{r}(s))ds\right)$$

로 계산할 수 있다. 여기서 $$T(t)$$는 accumulated transmittance라는 이름이 붙은 함수이다. 이는 광선이 $$t_n$$에서 $$t$$까지를 어떤 입자와 부딪쳐서 막혀 사라지지 않고 통과할 확률을 의미한다. 적분 안의 항은

* 그 위치의 입자들이 방사하는 빛의 세기 및 색깔을 나타내는 $$\mathbf{c}(\mathbf{r}(t), \mathbf{d})$$에
* 해당 위치의 volume density $$ \sigma(\mathbf{r}(t))$$를 가중치 삼아 곱해준 후
* 그 광선이 카메라가 있는 지점까지 막히지 않고 도달할 확률 $$ T(t)$$를 곱한 것

으로 이해할 수 있다. 한편, $$T(t)$$의 경우 

$$ \frac{1}{T}\frac{dT}{dt} = -\sigma(\mathbf{r}(t)) $$

일 것이므로 어렵지 않게 유도가 가능하다. ($$d$$이 단위벡터이므로 $$\vert d\mathbf{r}\vert = dt$$이다) 

$$C(\mathbf{r})$$을 적분을 통해 계산하기 위해서는 

$$t_i \sim U\left[t_n + \frac{i-1}{N}(t_f-t_n), t_n+\frac{i}{N}(t_f-t_n)\right]$$

으로 샘플링을 한 후 Riemann sum으로 구분구적법하듯이 적분값을 구하면 된다. (점을 등간격으로 잡는 것도 아니고, 아예 $$ U(t_n, t_f)$$에서 $$N$$개를 샘플링하는 것도 아니고 구간을 쪼갠 후 각각에서 랜덤하게 샘플링해주는 이유는 잘 모르겠다.)

식으로 나타내자면 다음과 같다.

$$ \hat{C}(\mathbf{r})  = \sum_{i=1}^N T_i(1-\text{exp}(-\sigma_i \delta_i))\mathbf{c_i}$$

$$ T_i = \text{exp}\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right)\quad\quad \delta_i = t_{i+1}-t_i$$

# NeRF의 훈련
앞서 이야기한 내용을 정리하자면 다음과 같다.
* NeRF는 5D input $$(\mathbf{x}, \mathbf{d})$$를 받아서 색깔과 volume density를 내보낸다.
* 이를 volume rendering 테크닉을 이용해 2D 사진으로 구성해낸다.
* 구성해낸 사진과 ground truth의 차이를 구해, loss function으로 삼아 트레이닝시킨다.

그런데 실제로는 이것만으로는 충분히 좋은 해상도의 사진을 얻을 수 없기 때문에 저자들은 두 가지 개선점을 도입하였다. 

## Positional Encoding
Positional Encoding은 자연어처리에서 많이 사용되는 방법이다. 문장을 Transformer와 같은 모델에 넣어줄 때, 단어간의 순서 관계를 넣어주기 위해서 

$$\gamma(p) = \left(\sin(2^0\pi p), \cos(2^0\pi p),\cdots, \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p)\right)$$

과 같은 벡터를 더해주는 방법이다. 

<p align="center" style="color:gray">
<img src="https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png" width="80%"/><br>
Positional Encoding을 시각화한 것. 세로축이 p에 해당한다. (<a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">출처</a>)
<br>
</p>
NeRF에서는 무언가의 순서관계를 위한 것은 아니고, 다른 목적으로 사용된다. 기본적으로 MLP는 연속함수를 근사한다. 우리가 렌더링하려는 물체가 아주 세밀한 구조를 가진다면, 우리의 MLP는 위치와 각도에 따라 출력이 매우 빠르게 변해야 하고(high-frequency variation) 이러한 함수는 MLP가 잘 근사할 수 없게 된다. 이것을 해결하기 위해서 우리는 MLP에 $$\mathbf{x}$$와 $$\mathbf{d}$$를 넣어주는 대신, $$\gamma(\mathbf{x})$$와 $$\gamma(\mathbf{d})$$를 넣어줄 것이다. 이렇게 하면 입력 자체에 $$\mathbf{x}$$와 $$\mathbf{d}$$에 대한 high-frequency term들이 들어가므로, 이들을 적절하게 변환한 결과인 MLP의 출력 또한 high-frequency variation을 잘 표현할 수 있게 된다.

## Hierarchical Sampling
ACM 논문 버전에서는 생략했지만, arXiv의 원문을 보면 Hierarchical sampling이라는 것을 도입했다고 언급하고 있다. 간단하게 말하자면, $$t$$를 샘플링하여 $$\hat{C}$$를 계산할 때 위에서 설명한 것처럼 naïve하게 하지 않고 이를 위해 네트워크를 미리 대충(coarsely) 훈련시켜 놓는 것이다. 실제 NeRF의 구현에서는 먼저 coarse network를 위에서 설명한 방식으로 훈련시켜서, 물체의 대략적인 volume density에 대한 정보를 얻는다. 이후 fine network를 트레이닝할 때에는 $$t$$를 샘플링할 때는 coarse network가 알려주는 volume density의 분포를 토대로, volume density가 높은 곳에서는 $$t$$를 많이 샘플링하고 그렇지 않은 곳에서는 적게 샘플링하는 방식을 사용한다. 이를 통해 렌더링을 위해 중요한 부분들을 위주로 샘플링을 함으로써 해상도의 향상을 이루어낼 수 있다.


# Results

<p align="center" style="color:gray">
<img src="/assets/img/NeRF/fig4.jpeg" width="90%"/>
<br>
NeRF와 기존 방법들의 정성적 비교. 맨 왼쪽 열이 ground truth(실제), 두 번째가 NeRF의 결과이고 나머지는 기존의 방법들의 결과 사진이다.
</p>

위 사진에서 볼 수 있듯이 정성적인 비교에서도 NeRF는 기존의 모델들과 비교하여 압도적인 성능을 보여주었으며, 미세한 구조나 섬세한 재질의 표현이 필요한 경우에도 높은 해상도의 이미지를 합성해내는 것을 확인할 수 있었다.

<p align="center" style="color:gray">
<img src="/assets/img/NeRF/fig5.jpg" width="90%"/>
<br> NeRF(맨 아래 행)와 기존 방법들의 정량적 비교. pSNR과 SSIM은 높을수록 좋고, LPIPS는 낮을수록 좋은 것이다
</p>

정량적 비교에서도 NeRF는기존 SOTA 모델보다 좋은 성능을 보여준다. 또한, 트레이닝하는 데 걸리는 시간이 긴 대신에 하나의 scene에 대한 파라미터들을 저장하는 데 5MB 이하의 메모리만으로 충분하다는 압도적인 장점을 가진다. 

# 결론
NeRF는 물체와 그 모습을 MLP를 사용해 연속함수로 근사하려던 기존 방법들의 단점을 해결하여, 공간 상의 좌표와 관찰 각도를 입력으로 받아 view-dependent emitted radiance와 volume density를 출력으로 하는 MLP, **neural radience field(NeRF)**를 정의하였다. 이는 Deep CNN을 사용해 물체를 voxel로 이산적으로 표현하려던 기존의 주류 방법론에 비해 우세한 성능을 보여주었다.

# 참고문헌
<a href="https://www.matthewtancik.com/nerf">프로젝트 사이트 </a>

