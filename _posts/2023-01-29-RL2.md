---
layout: post
title: "[강화학습] 2. Learning Policy"
date: 2023-01-29 01:00:00 +0900
description: 강화학습의 수학적 기반을 살펴본다
tags: deep-learning reinforcement-learning
giscus_comments: true
---

<a href="/blog/2023/RL/">전의 글</a>에서는 강화학습 환경을 모델링할 수 있는 Markov Decision Process의 개념을 살펴보았고 Optimal Policy를 정의하였다. 또한, Optimal state-value/action function이 만족해야 하는 Bellman Optimality Equation을 살펴보았다. 이번 포스팅에서는 실제로 policy를 iterative한 방법을 통해 계산하는 방법을 알아본다.

# Policy/Value Iteration

## Policy Iteration

**Policy Iteration**은 크게 **Policy evaluation**과 **Policy improvement**의 과정으로 나뉠 수 있다. 먼저 Policy evaluation은 policy $$\pi$$가 주어질 떄, 이에 대응되는 state-value function $$v^\pi$$를 구하는 것을 말한다. 앞선 포스팅에서, state value-function을 행렬 형태로 나타내면

$$ v^\pi = R^\pi + \gamma P^\pi v^\pi$$
 
와 같은 방정식을 만족한다는 것을 유도했었다. 물론 inverse를 사용해서 $$ v^\pi = (I -\gamma P^\pi )^{-1}R^\pi$$ 형태로 만들어줄 수 있다면 좋겠지만 inverse를 직접 계산하는 것은 수치적으로 불안정할 뿐만 아니라 계산이 오래 걸리기 때문에 대신해서 반복법을 사용한다. 방법은 연립방정식을 풀 때 <a href="https://en.wikipedia.org/wiki/Jacobi_method"> Jacobi Method</a>를 사용하는 것과 동일하다. 

* $$v_1(s)$$를 임의의 값으로 초기화하기
* $$v_k$$로부터 $$v_{k+1} = R^{\pi}(s) + \gamma \sum_{s'\in\mathcal{S}}\mathcal{P}^\pi(s, s') \cdot v_k(s')$$를 통해 $$v_{k+1}$$를 계산하기

이를 충분히 오래 반복하면, 즉 $$k\to \infty$$로 갈때 $$v_k$$가 만약 수렴한다면 $$\lim_{k\to \infty}v_k = v^\pi$$ 는 당연히 

$$ v^\pi = R^\pi + \gamma P^\pi v^\pi$$

의 해가 될 것이다. 그렇다면 $$v_k$$가 수렴한다는 사실만 확실하면 되는데, 이는 초기값에 상관없이 수렴한다는 것이 알려져 있다. 즉 위의 방법을 사용하면 주어진 policy로부터 대응되는 state-value function을 얻어낼 수 있다.

이제 policy에 대응되는 state-value function을 알았다면 이를 토대로 policy를 수정하는 과정을 거친다. 이것을 **policy improvement**라고 한다. $$l$$번째 policy $$\pi_l$$로부터 개선을 이루어내 $$\pi_{l+1}$$을 계산하는 것이다.

Policy improvment를 수행하는 방법 또한 우리의 직관에서 크게 벗어나지 않는다. 우선 전 게시물에서, optimal policy는 단순히 deterministic하다고 가정해도 상관이 없다고 했던 것을 기억할 것이다. 따라서 optimal policy $$\pi^*$$와, 이에 대응되는 action-value function $$q^{\pi^*}$$는

$$\pi^*(s) = \text{argmax}_{a\in\mathcal{A}}q^{\pi^*}(s, a)$$

를 만족해야 한다. 그런데 현재의 단계에서는 이것이 성립하지 않으므로 policy evaluation에서 썼던 것과 유사한 논리로 iteration을 통해 성립하도록 해주면 되는 것이다. 

$$\pi_{l+1}(s) = \text{argmax}_{a\in\mathcal{A}}q^{\pi_{l}}(s, a)$$

여기에서 $$q^{\pi_l}$$은 $$v^{\pi_l}$$로부터 

$$ q^{\pi_l}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot v^{\pi_l}(s’) $$

로 구해주면 된다. 

이렇게 policy iteration은 policy evaluation과 improvement를 번갈아가면서 반복해주면 된다.

* $$\pi_1$$를 임의의 값으로 설정
* $$\pi_1$$로부터 **policy evaluation**을 시행해서 $$\pi_1$$에 대응되는 state-value function $$v^{\pi_1}$$을 계산
* $$v^{\pi_1}$$으로부터 **policy improvement**를 시행해서 $$\pi_2$$를 계산

$$\vdots$$

* $$\pi_l$$로부터 **policy evaluation**을 시행해서 $$\pi_l$$에 대응되는 state-value function $$v^{\pi_l}$$을 계산
* $$v^{\pi_l}$$으로부터 **policy improvement**를 시행해서 $$\pi_{l+1}$$를 계산

이렇게 반복을 거치면, 

$$\lim_{l\to\infty}\pi_l = \pi^*$$

로 수렴함이 증명되어 있다. 이렇게 해서 optimal policy를 구할 수 있었다.

## Value Iteration

**Value iteration** 또한 유사하게 반복법을 사용해서 optimal policy를 계산하지만, policy를 계산하지 않고 value만으로 iteration을 돌린 후에 optimal state-value function $$v^*$$를 얻어 그것으로 $$\pi^*$$를 계산하는 방식이다.

Value iteration에서는 Bellman Optimality Equation중, optimal state-value function에 대한 식 하나만을 사용한다.

$$ v^*(s)=\max _{a \in \mathcal{A}}\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot v^*\left(s^{\prime}\right)\right) $$

이것을 점화식

$$ v_{k+1}(s)=\max _{a \in \mathcal{A}}\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot v_k\left(s^{\prime}\right)\right) $$

으로 바꾸어서 위와 같은 방법으로 해를 구해주면 된다. $$v_1(s)$$은 임의로 정해준 후, 위의 점화식을 반복해서 적용해주는 것이다. 이때도 마찬가지로 $$v_k$$가 $$v^*$$로 수렴한다는 것을 증명할 수 있다.

Policy/Value Iteration은 Markov decision process $$(\mathcal{S, A, P}, R, \gamma)$$에 대한 정보가 모두 알려져 있다면 매우 효과적으로 적용할 수 있다. 다만 $$\mathcal{P}$$와 $$R$$을 model이라고 부르는데, model에 대한 정보를 미리 알고있는 것은 굉장히 강한 조건이다. Model을 모르는 상태에서는 Bellman Optimality Equation을 점화식으로 바꿔 반복하는 위의 방법들을 적용하기가 어렵다. 또, $$\mathcal{S}$$나 $$\mathcal{A}$$의 크기가 크면 적용하기가 어렵다는 단점이 있다.

이렇게 Model을 미리 모르는 상태에서 optimal policy를 계산하는 것을 **model-free learning**이라고 한다. model-free learning을 위해서는 다른 알고리즘을 사용해야 한다.

# Model-Free Learning
## Monte-Carlo Learning

Model-Free learning의 가장 간단한 방법으로는 **Monte-Carlo Iteration**이 있다. Model이 주어져 있지 않을 때, Policy Iteration에서 문제가 되는 부분은 transition probability $$\mathcal{P}$$를 알 수 없기 떄문에 state-value function을 얻는 policy evaluation이 불가능하다는 점이다. Monte-Carlo iteration에서는 policy evaluation 과정에서 $$v^{\pi_l}$$ 대신 $$q^{\pi_l}$$을 사용하며, $$q^{\pi_l}$$의 정확한 값을 얻는 것을 포기하고 대신 그 근사값을 사용한다. 주어진 policy $$\pi_l$$을 따라 액션을 취하면서 state들을 쭉 따라가며 reward를 계산하고, 반복해서 평균을 내는 것이다. 이렇게 해서 $$q^{\pi_l}$$의 근사값을 대략적으로는 구해낼 수가 있다.

Policy improvement의 경우 기존과 거의 유사하지만,  **exploration**이라는 것을 도입한다. 또한, policy evaluation에서부터 $$v^{\pi_l}$$이 아닌 $$q^{\pi_l}$$을 계산했기 때문에 $$q^{\pi_l}$$를 구하는 계산은 따로 해주지 않아도 되며, 그 덕분에 model이 없어도 계산이 가능하다. Monte Carlo policy improvement에서는 작은 수 $$\epsilon>0$$을 설정하고, $$1-\epsilon$$의 확률로는 기존과 마찬가지로 action-value가 가장 큰 액션($$\text{argmax}_{a'\in \mathcal{A}} q^{\pi_l}(s, a')$$)을 하지만, $$\epsilon$$의 작은 확률로는 그냥 아무 액션이나 해보는 것이다. 전자를 **exploitation**, 후자를 **exploration**이라고 한다. 다양한 경험을 해보기 위해서 모험을 한다는 뜻이다. 식으로 표현하자면 다음과 같다.

$$ \pi_{\ell+1}(s, a)= \begin{cases}\epsilon /|\mathcal{A}|+1-\epsilon & \text { if } a=\operatorname{argmax}_{a^{\prime} \in \mathcal{A}} q^{\pi_{\ell}}\left(s, a^{\prime}\right) \\ \epsilon /|\mathcal{A}| & \text { otherwise }\end{cases}$$

## Temporal-Differnce Learning
Temporal-Difference Learning은 Monte-Carlo Learning에서 다이나믹 프로그래밍의 요소를 도입해서 state sequence의 끝까지 가보지 않고도 $$q^{\pi_l}$$을 근사하는 방법이다. 

### SARSA
### Q-learning




# 참고문헌
Lecture 06: Reinforcement Learning, CS Semiar (Machine Learning in Practice) lecture slides, Korea Science Academy of KAIST