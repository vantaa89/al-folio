<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>[강화학습] 2. Learning Policy | Seojune Lee</title> <meta name="author" content="Seojune Lee"> <meta name="description" content="강화학습의 수학적 기반을 살펴본다"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https:/vantaa89.github.io//blog/2023/RL2/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Seojune </span>Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">[강화학습] 2. Learning Policy</h1> <p class="post-meta">January 28, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fas fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fas fa-hashtag fa-sm"></i> reinforcement-learning</a>   </p> </header> <article class="post-content"> <p><a href="/blog/2023/RL/">전의 글</a>에서는 강화학습 환경을 모델링할 수 있는 Markov Decision Process의 개념을 살펴보았고 Optimal Policy를 정의하였다. 또한, Optimal state-value/action function이 만족해야 하는 Bellman Optimality Equation을 살펴보았다. 이번 포스팅에서는 실제로 policy를 iterative한 방법을 통해 계산하는 방법을 알아본다.</p> <h1 id="policyvalue-iteration">Policy/Value Iteration</h1> <h2 id="policy-iteration">Policy Iteration</h2> <p><strong>Policy Iteration</strong>은 크게 <strong>Policy evaluation</strong>과 <strong>Policy improvement</strong>의 과정으로 나뉠 수 있다. 먼저 Policy evaluation은 policy \(\pi\)가 주어질 떄, 이에 대응되는 state-value function \(v^\pi\)를 구하는 것을 말한다. 앞선 포스팅에서, state value-function을 행렬 형태로 나타내면</p> \[v^\pi = R^\pi + \gamma P^\pi v^\pi\] <p>와 같은 방정식을 만족한다는 것을 유도했었다. 물론 inverse를 사용해서 \(v^\pi = (I -\gamma P^\pi )^{-1}R^\pi\) 형태로 만들어줄 수 있다면 좋겠지만 inverse를 직접 계산하는 것은 수치적으로 불안정할 뿐만 아니라 계산이 오래 걸리기 때문에 대신해서 반복법을 사용한다. 방법은 연립방정식을 풀 때 <a href="https://en.wikipedia.org/wiki/Jacobi_method" rel="external nofollow noopener" target="_blank"> Jacobi Method</a>를 사용하는 것과 동일하다.</p> <ul> <li>\(v_1(s)\)를 임의의 값으로 초기화하기</li> <li>\(v_k\)로부터 \(v_{k+1} = R^{\pi}(s) + \gamma \sum_{s'\in\mathcal{S}}\mathcal{P}^\pi(s, s') \cdot v_k(s')\)를 통해 \(v_{k+1}\)를 계산하기</li> </ul> <p>이를 충분히 오래 반복하면, 즉 \(k\to \infty\)로 갈때 \(v_k\)가 만약 수렴한다면 \(\lim_{k\to \infty}v_k = v^\pi\) 는 당연히</p> \[v^\pi = R^\pi + \gamma P^\pi v^\pi\] <p>의 해가 될 것이다. 그렇다면 \(v_k\)가 수렴한다는 사실만 확실하면 되는데, 이는 초기값에 상관없이 수렴한다는 것이 알려져 있다. 즉 위의 방법을 사용하면 주어진 policy로부터 대응되는 state-value function을 얻어낼 수 있다.</p> <p>이제 policy에 대응되는 state-value function을 알았다면 이를 토대로 policy를 수정하는 과정을 거친다. 이것을 <strong>policy improvement</strong>라고 한다. \(l\)번째 policy \(\pi_l\)로부터 개선을 이루어내 \(\pi_{l+1}\)을 계산하는 것이다.</p> <p>Policy improvment를 수행하는 방법 또한 우리의 직관에서 크게 벗어나지 않는다. 우선 전 게시물에서, optimal policy는 단순히 deterministic하다고 가정해도 상관이 없다고 했던 것을 기억할 것이다. 따라서 optimal policy \(\pi^*\)와, 이에 대응되는 action-value function \(q^{\pi^*}\)는</p> \[\pi^*(s) = \text{argmax}_{a\in\mathcal{A}}q^{\pi^*}(s, a)\] <p>를 만족해야 한다. 그런데 현재의 단계에서는 이것이 성립하지 않으므로 policy evaluation에서 썼던 것과 유사한 논리로 iteration을 통해 성립하도록 해주면 되는 것이다.</p> \[\pi_{l+1}(s) = \text{argmax}_{a\in\mathcal{A}}q^{\pi_{l}}(s, a)\] <p>여기에서 \(q^{\pi_l}\)은 \(v^{\pi_l}\)로부터</p> \[q^{\pi_l}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot v^{\pi_l}(s’)\] <p>로 구해주면 된다.</p> <p>이렇게 policy iteration은 policy evaluation과 improvement를 번갈아가면서 반복해주면 된다.</p> <ul> <li>\(\pi_1\)를 임의의 값으로 설정</li> <li>\(\pi_1\)로부터 <strong>policy evaluation</strong>을 시행해서 \(\pi_1\)에 대응되는 state-value function \(v^{\pi_1}\)을 계산</li> <li>\(v^{\pi_1}\)으로부터 <strong>policy improvement</strong>를 시행해서 \(\pi_2\)를 계산</li> </ul> \[\vdots\] <ul> <li>\(\pi_l\)로부터 <strong>policy evaluation</strong>을 시행해서 \(\pi_l\)에 대응되는 state-value function \(v^{\pi_l}\)을 계산</li> <li>\(v^{\pi_l}\)으로부터 <strong>policy improvement</strong>를 시행해서 \(\pi_{l+1}\)를 계산</li> </ul> <p>이렇게 반복을 거치면,</p> \[\lim_{l\to\infty}\pi_l = \pi^*\] <p>로 수렴함이 증명되어 있다. 이렇게 해서 optimal policy를 구할 수 있었다.</p> <h2 id="value-iteration">Value Iteration</h2> <p><strong>Value iteration</strong> 또한 유사하게 반복법을 사용해서 optimal policy를 계산하지만, policy를 계산하지 않고 value만으로 iteration을 돌린 후에 optimal state-value function \(v^*\)를 얻어 그것으로 \(\pi^*\)를 계산하는 방식이다.</p> <p>Value iteration에서는 Bellman Optimality Equation중, optimal state-value function에 대한 식 하나만을 사용한다.</p> \[v^*(s)=\max _{a \in \mathcal{A}}\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot v^*\left(s^{\prime}\right)\right)\] <p>이것을 점화식</p> \[v_{k+1}(s)=\max _{a \in \mathcal{A}}\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot v_k\left(s^{\prime}\right)\right)\] <p>으로 바꾸어서 위와 같은 방법으로 해를 구해주면 된다. \(v_1(s)\)은 임의로 정해준 후, 위의 점화식을 반복해서 적용해주는 것이다. 이때도 마찬가지로 \(v_k\)가 \(v^*\)로 수렴한다는 것을 증명할 수 있다.</p> <p>Policy/Value Iteration은 Markov decision process \((\mathcal{S, A, P}, R, \gamma)\)에 대한 정보가 모두 알려져 있다면 매우 효과적으로 적용할 수 있다. 다만 \(\mathcal{P}\)와 \(R\)을 model이라고 부르는데, model에 대한 정보를 미리 알고있는 것은 굉장히 강한 조건이다. Model을 모르는 상태에서는 Bellman Optimality Equation을 점화식으로 바꿔 반복하는 위의 방법들을 적용하기가 어렵다. 또, \(\mathcal{S}\)나 \(\mathcal{A}\)의 크기가 크면 적용하기가 어렵다는 단점이 있다.</p> <p>이렇게 Model을 미리 모르는 상태에서 optimal policy를 계산하는 것을 <strong>model-free learning</strong>이라고 한다. model-free learning을 위해서는 다른 알고리즘을 사용해야 한다.</p> <h1 id="model-free-learning">Model-Free Learning</h1> <h2 id="monte-carlo-learning">Monte-Carlo Learning</h2> <p>Model-Free learning의 가장 간단한 방법으로는 <strong>Monte-Carlo Iteration</strong>이 있다. Model이 주어져 있지 않을 때, Policy Iteration에서 문제가 되는 부분은 transition probability \(\mathcal{P}\)를 알 수 없기 떄문에 state-value function을 얻는 policy evaluation이 불가능하다는 점이다. Monte-Carlo iteration에서는 policy evaluation 과정에서 \(v^{\pi_l}\) 대신 \(q^{\pi_l}\)을 사용하며, \(q^{\pi_l}\)의 정확한 값을 얻는 것을 포기하고 대신 그 근사값을 사용한다. 주어진 policy \(\pi_l\)을 따라 액션을 취하면서 state들을 쭉 따라가며 reward를 계산하고, 반복해서 평균을 내는 것이다. 이렇게 해서 \(q^{\pi_l}\)의 근사값을 대략적으로는 구해낼 수가 있다.</p> <p>Policy improvement의 경우 기존과 거의 유사하지만, <strong>exploration</strong>이라는 것을 도입한다. 또한, policy evaluation에서부터 \(v^{\pi_l}\)이 아닌 \(q^{\pi_l}\)을 계산했기 때문에 \(q^{\pi_l}\)를 구하는 계산은 따로 해주지 않아도 되며, 그 덕분에 model이 없어도 계산이 가능하다. Monte Carlo policy improvement에서는 작은 수 \(\epsilon&gt;0\)을 설정하고, \(1-\epsilon\)의 확률로는 기존과 마찬가지로 action-value가 가장 큰 액션(\(\text{argmax}_{a'\in \mathcal{A}} q^{\pi_l}(s, a')\))을 하지만, \(\epsilon\)의 작은 확률로는 그냥 아무 액션이나 해보는 것이다. 전자를 <strong>exploitation</strong>, 후자를 <strong>exploration</strong>이라고 한다. 다양한 경험을 해보기 위해서 모험을 한다는 뜻이다. 식으로 표현하자면 다음과 같다.</p> \[\pi_{\ell+1}(s, a)= \begin{cases}\epsilon /|\mathcal{A}|+1-\epsilon &amp; \text { if } a=\operatorname{argmax}_{a^{\prime} \in \mathcal{A}} q^{\pi_{\ell}}\left(s, a^{\prime}\right) \\ \epsilon /|\mathcal{A}| &amp; \text { otherwise }\end{cases}\] <h2 id="temporal-differnce-learning">Temporal-Differnce Learning</h2> <p>Temporal-Difference Learning은 Monte-Carlo Learning에서 다이나믹 프로그래밍의 요소를 도입해서 state sequence의 끝까지 가보지 않고도 \(q^{\pi_l}\)을 근사하는 방법이다.</p> <h3 id="sarsa">SARSA</h3> <h3 id="q-learning">Q-learning</h3> <h1 id="참고문헌">참고문헌</h1> <p>Lecture 06: Reinforcement Learning, CS Semiar (Machine Learning in Practice) lecture slides, Korea Science Academy of KAIST</p> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Seojune Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>