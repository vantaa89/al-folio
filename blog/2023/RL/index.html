<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>[강화학습] 1. Markov Decision Process, Optimal Policy | Seojune Lee</title> <meta name="author" content="Seojune Lee"> <meta name="description" content="강화학습의 수학적 기반을 살펴본다"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https:/vantaa89.github.io//blog/2023/RL/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Seojune </span>Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">[강화학습] 1. Markov Decision Process, Optimal Policy</h1> <p class="post-meta">January 26, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fas fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fas fa-hashtag fa-sm"></i> reinforcement-learning</a>   </p> </header> <article class="post-content"> <h1 id="들어가는-글">들어가는 글</h1> <p>보통 머신러닝을 처음 배울 때 머신러닝의 종류는 세 가지가 있다고 배운다. 지도학습, 비지도학습, 그리고 강화학습이다. 그런데 많은 머신러닝/딥러닝 입문 수업에서 지도학습과 비지도학습은 어느 정도 다루지만 강화학습은 다루지 않는 경우도 많은 것 같다. 이 때문에 강화학습을 접할 기회가 많지 않은 것 같아 이 포스팅에서 강화학습을 한번 살펴보겠다. 먼저 강화학습의 정의를 살펴보자.</p> <blockquote> <p><strong>Reinforcement Learning(강화학습)</strong>: 어떤 환경 안에서 정의된 에이전트가 현재의 상태를 인식하여, 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법 (위키백과)</p> </blockquote> <p>간단한 예제로 한번 살펴보자면, Pong(탁구) 게임을 플레이하는 에이전트를 생각해볼 수 있을 것이다.</p> <p align="center" style="color:gray"> <img src="https://i.guim.co.uk/img/static/sys-images/Technology/Pix/pictures/2008/04/16/Pong460x276.jpg?width=465&amp;quality=85&amp;dpr=1&amp;s=none"><br> Pong </p> <p>이 경우 각각은 다음과 같다.</p> <ul> <li>input: 게임의 상태. 픽셀값</li> <li>output: 플레이어가 해야 하는 최적의 행동. 입력이 주어질 때 이것을 학습해야 한다.</li> <li>환경(environment): 게임 프로그램</li> <li>에이전트: 게임을 하는 플레이어. 에이전트가 바를 어떻게 움직이냐에 따라서 환경은 이에 반응해서 점수가 나오게 될 것이다.</li> <li>보상(reward): 점수의 변화량</li> </ul> <p>지도학습의 경우 offline에 input과 output이 주어지는 데 비해서, 비지도학습의 경우에는 output이 주어지지 않고 input의 구조를 파악하는 데에 집중한다. 반면 강화학습은 현재의 상태를 보고 어떤 행동을 해야할지 훈련하는 과정에서, 주변 환경과의 상호작용으로 시간이 지난 후 행동에 따른 보상이나 페널티가 어떻게 나왔는지를 기반으로 학습을 하게 된다. 이를 학습하여 에이전트는 어떤 행동을 해야 보상을 최대화할 수 있는지를 훈련받게 된다.</p> <p>필자는 “정보과학세미나”라는 과목에서 강화학습을 한번 배운 적이 있지만, 당시 시간을 많이 투자하지 못하기도 했고 Tensorflow같은 프레임워크에 익숙하지 않은 채 수업을 들었었기 때문에 내용을 완전히 이해했다고 말하기는 힘든 상태였던 것 같다. 당시 배웠던 내용을 복습하며, 다른 책이나 자료들을 참고해 이 글로 정리해본다.</p> <h1 id="markov-reward-process">Markov Reward Process</h1> <p>먼저, Markov Process라는 개념을 정의하자.</p> <blockquote> <p><strong>Markov Process</strong>(Markov Chain)<br> A Markov process is a pair \((S, P)\) where</p> <ul> <li>\(\mathcal{S}\): a finite set of states</li> <li>\(\mathcal{P}\): \(S^2 \to [0, 1]\): a state transition probability</li> </ul> </blockquote> <p>즉, 마르코프 과정(또는 체인)은 유한한 상태들의 집합과, 각 상태에서 다음 상태로 갈 확률을 명시해놓은 개념이다.</p> <p align="center" style="color:gray"> <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Markovkate_01.svg/1920px-Markovkate_01.svg.png" width="40%"><br> Markov Process의 예시 </p> <p>위의 경우 첫 번째 행/열을 A, 두 번째 행/열을 E라고 한다면 transition probability \(P\)는 다음과 같이 행렬로 쓸 수 있다.</p> \[\mathcal{P} = \begin{pmatrix} 0.6 &amp; 0.4\\ 0.3 &amp; 0.7 \end{pmatrix}\] <p>Markov Process에서, 다음의 상태로 전이할 확률은 현재의 상태에만 의존하고, 그 전까지 상태들을 지나온 내역(history)에는 의존하지 않는다는 특징이 있다. 이를 식으로 표현하자면 다음과 같다. (\(S_t\): 시간 \(t\)에서의 state)</p> \[\mathbb{P}[S_{t+1}=s'|S_t=s, S_{t-1}, \cdots, S_1] = \mathbb{P}[S_{t+1}=s'|S_t=s] = \mathcal{P}(s, s')\] <p>이 Markov Process에서 <strong>reward function</strong>(보상 함수)과 discount factor를 추가하면 <strong>Markov Reward Process</strong>가 된다.</p> <blockquote> <p><strong>Markov Reward Process</strong><br> A Markov reward process is a tuple \((\mathcal{S}, \mathcal{P}, R, \gamma)\) where</p> <ul> <li>\((\mathcal{S}, \mathcal{P})\): a Markov process</li> <li>\(R: \mathcal{S}\to \mathbb{R}\): a reward function</li> <li>\(\gamma\): a discount factor</li> </ul> </blockquote> <p>여기에서 \(R\)은 현재의 state에 대한 보상이라고 생각할 수 있다. 어떤 state에 도달하기만 해도, \(R\)이라는 함수가 이에 대한 보상을 부여한다고 생각을 하자. 이것을 토대로 <strong>State-Value Function</strong>이라는 것을 정의한다. 이름에서 알 수 있듯이, 각 state에 대응하는 __가치(value)__를 부여하는 함수이다.</p> <blockquote> <p>Given an MRP \((\mathcal{S}, \mathcal{P}, R, \gamma)\), its <strong>state-value function</strong> \(v: \mathcal{S} \to \mathbb{R}\) is</p> <ul> <li>\(v(s) = \mathbb{E}[\sum_{k=0}^\infty \gamma^k R(N_k(s))]\) <br> where \(N_k(s)\) is the random variable describing the state after $k$ steps from \(s\).</li> </ul> </blockquote> <p>state-value function은 미래에 받을 보상까지도 모두 기댓값으로 계산해 고려한 것이라고 할 수 있다. 물론 미래에 받을 보상을 현재에 받을 보상과 똑같이 취급하지는 않고, \(k\)번의 스텝 후에 받을 보상은 \(\gamma^k\)만큼 절하해서 고려하는 것이다. \(\gamma\)가 낮을수록 현재의 보상에 더 충실해질 것이고, 높을수록 현재를 조금 희생하더라도 미래에 큰 보상을 받는 방향을 추구하게 될 것이다.</p> <p>그런데 \(v\)는 계산 과정에서 무한급수가 등장하므로 사용이 불편하다. 식을 정리해서 무한급수가 등장하지 않도록 해보자.</p> \[v(s) = \mathbb{E} \left[\sum_{k=0}^\infty \gamma^k R(N_k(s)) \right]\] \[= \mathbb{E}\left[ R(s) + \gamma \sum_{k=0}^\infty \gamma^k R(N_k(N(s))) \right]\] \[= \mathbb{E}\left[R(s) + \gamma \cdot v(N(s)) \right]\] \[= R(s) + \gamma \cdot \sum_{s'\in \mathcal{S}} \left(\mathcal{P}(s, s') v(s')\right)\] <p>이를 행렬로 나타내면 다음과 같다.</p> \[\left(\begin{array}{c}v\left(s_1\right) \\ \vdots \\ v\left(s_n\right)\end{array}\right)=\left(\begin{array}{c}R\left(s_1\right) \\ \vdots \\ R\left(s_n\right)\end{array}\right)+\gamma\left(\begin{array}{ccc}\mathcal{P}\left(s_1, s_1\right) &amp; \ldots &amp; \mathcal{P}\left(s_1, s_n\right) \\ \vdots &amp; \ddots &amp; \vdots \\ \mathcal{P}\left(s_n, s_1\right) &amp; \ldots &amp; \mathcal{P}\left(s_n, s_n\right)\end{array}\right)\left(\begin{array}{c}v\left(s_1\right) \\ \vdots \\ v\left(s_n\right)\end{array}\right)\] <p>즉,</p> \[v= R + \gamma \mathcal{P} v\] <p>이므로</p> \[v = (1-\gamma \mathcal{P})^{-1}R\] <p>이 된다.</p> <h1 id="markov-decision-process">Markov Decision Process</h1> <p>이번에는 <strong>Markov Decision Process</strong>라는 것을 정의한다. 비슷한 용어가 계속 나오지만 헷갈리지 말자. 사실 이걸 정의하기 위해서 앞의 개념들이 나온 것이다.</p> <blockquote> <p><strong>Markov Decision Process</strong><br> A Markov decision process is a tuple \((\mathcal{S}, \mathcal{A}, \mathcal{P}, R, \gamma)\) where</p> <ul> <li>\(\mathcal{S}\): a finite set of states</li> <li>\(\mathcal{A}\): a finite set of <strong>actions</strong> </li> <li>\(\mathcal{P}: \mathcal{S\times A\times S}\to [0, 1]\): a state transition probability</li> <li>\(R: \mathcal{S\times A}\to \mathbb{R}\): a reward function</li> <li>\(\gamma \in [0, 1]\): a discount factor</li> </ul> </blockquote> <p>정의를 보면 action이라는 개념이 새로 나온 것을 제외하면 앞의 Markov reward process와 크게 다르지 않다. 또, MRP에서는 현재의 state가 주어지면 다음 state가 무엇이 될지는 확률적으로 결정될 뿐이었는 데 비해서 MDP에서는 \(P\)가 action에도 의존하는 것이 차이점이다. 즉, <strong>에이전트가 어떤 액션을 가해서 다음 state가 무엇이 될 지 어느 정도 영향을 미칠 수 있는 것이다</strong>. Reward 또한 정의가 바뀌었다. 이제 \(R(s, a)\)는 다음과 같이 정의한다.</p> <blockquote> <p>\(R(s, a)\): 현재 state \(s\)에서 \(a\)를 택했을 때, 다음 state들에서 받을 수 있는 expected reward</p> </blockquote> <p>즉 보상 함수는 현재의 state에 따라서 부여되는 것이 아니라, 현재의 state에서 어떤 action을 취하냐에 따라서도 달라지게 된다는 것이 차이점이다.</p> <p><strong>강화학습의 대상이 되는 문제들은 대부분 environment가 Markov decision process로 나타낼 수 있는 것들이다</strong>.</p> <p>이제 주어진 MDP \((\mathcal{S, A, P}, R, \gamma)\) 에 대해 <strong>Policy</strong>(정책)라는 것을 정의한다.</p> <blockquote> <p><strong>Policy</strong><br> A policy of an MDP \((\mathcal{S, A, P}, R, \gamma)\) is a probability distribution over actions given states: \(\pi: \mathcal{S \times A} \to [0, 1]\)</p> </blockquote> <p>말이 어려워보이지만 간단히 말해서, 현재의 상태를 보고 에이전트가 어떤 액션을 취할지 결정하는 함수가 바로 policy이다. 이렇게 생각하면 policy(정책)이라는 이름이 잘 지어진 것이라고 느껴진다. 즉, 현재의 state에 따라서 어떤 action이 취해질지가 (확률적으로) 결정되고, 이렇게 결정된 action은 다시 현재의 state와 함께 다음의 state를 확률적으로 결정하는 데 사용된다.</p> <p>MDP \((\mathcal{S, A, P}, R, \gamma)\)와 Policy \(\pi\)가 주어져있다면 state끼리 이동하는 과정은 그냥 Markov Process \((\mathcal{S}, \mathcal{P}^\pi)\) 가 된다. 당연한 것이, 현재의 state를 보고 어떤 action을 취할지가 \(\pi\)에 다 명시되어 있고, 이 action에 따라 다음 state가 뭐가 될 지는 \(\mathcal{P}\)에 나와있기 떄문이다. 이때 현재 state \(s\)에서 다음 state \(s'\)으로 transition할 확률</p> \[\mathcal{P}^\pi(s, s') = \sum_{a\in \mathcal{A}} \pi(s, a) \cdot \mathcal{P} (s, a, s')\] <p>이다. 조금 생각해보면 어렵지 않게 받아들여진다. 또한,</p> \[\mathcal{R}^\pi(s) = \sum_{a\in \mathcal{A}} \pi(s, a) \cdot R(s, a)\] <p>로 정의하면 \((\mathcal{S, P^\pi, R^\pi}, \gamma)\)는 Markov reward process이다.</p> <p>강화학습의 목표는 이 policy를 최적화하는 것이 된다. 즉 현재의 상태(state)를 보고 에이전트가 어떤 액션을 취할지를 최적화하여 보상을 최대화하는 것이 목표이다.</p> <p>한편, 아까 MRP를 정의하면서 각 state에 가치를 부여하는 state-value function이라는 것이 있었던 것을 기억할 것이다. 그런데 이제 MDP에서는 다음의 state는 현재의 state에만 의존해서 결정되는 것이 아니다. action을 어떻게 취할지에 따라서 다음의 state가 달라지고, 또 에이전트가 받게 될 보상이 달라지는 것이다. 따라서 MDP에서는 우리가 취해줄 action 하나하나에 가치를 매겨주는 함수가 필요하다. 이름은 <strong>action-value function</strong>이라고 하면 알맞을 것이다.</p> <blockquote> <p><strong>Action-Value Function</strong><br> The action-value function \(q^\pi: \mathcal{S\times A}\to \mathbb{R}\) is given by</p> \[q^\pi(s, a) = R(s, a) + \gamma \sum_{s'\in \mathcal{S}}\left(\mathcal{P}(s, a, s') \cdot v^\pi (s'))\right)\] <p>where state-value function \(v^\pi\) is given by</p> \[v^\pi(s) = \mathbb{E}\left[ \sum_{k=0}^\infty \gamma^k R^\pi(N_k^\pi(s)) \right]\] </blockquote> <p>식이 어려워보이지만, 의미를 생각하면 크게 어렵지 않다. policy \(\pi\)가 이미 주어져 있으니, state가 주어지면 어떤 action을 취해야 할지는 확률분포가 바로 결정된다. 이 확률분포를 따라서 예상되는 state transition의 sequence를 확률변수로 나타낸 것이 \(N_k^\pi(s)\) 이다. 이것을 토대로 reward의 기댓값을 구하고, \(\gamma\)로 discount해준 것일 뿐이다.</p> <p>이제 대부분의 강화학습 환경을 모델링할 수 있는 Markov decision process를 알게 되었고, 현재의 상태를 보고 어떤 액션을 취해야 할지를 policy 함수로 결정하면 된다는 것도 알게 되었다. 또, 이러한 policy가 주어졌을 때 각각의 상태와 액션을 어떻게 평가해야 할지 또한 state-value function, action-value function을 정의함으로써 해결하였다. 이제 강화학습의 궁극적 목표인 policy를 결정하는 것만 해내면 된다. 그렇다면 어떤 policy가 좋은 policy일까?</p> <h1 id="optimizing-policy">Optimizing Policy</h1> <p>Optimal policy, 즉 가장 좋은 policy는 다름아닌 위에서 정의한 value function들(state-value function, action-value function)을 최적화하는 policy이다. 그렇다면 state-value function과 action-value function이 최적이라는 것이 무슨 의미인지부터 정의해야 할 것 이다.</p> <blockquote> <p><strong>Optimal State-Value Function</strong><br> The optimal state-value function \(v^*:\mathcal{S}\to \mathbb{R}\) is defined by</p> <ul> <li>\(v^*(s) = \mathrm{max}\{v^\pi(s) \vert \text{policy }\pi \text{ of the MDP}\}\) for each \(s \in \mathcal{S}\)</li> </ul> </blockquote> <blockquote> <p><strong>Optimal Action-Value Function</strong><br> The optimal action-value function \(q^*:\mathcal{S\times A}\to \mathbb{R}\) is defined by</p> <ul> <li>\(q^*(s, a) = \mathrm{max}\{q^\pi(s, a) \vert \text{policy }\pi \text{ of the MDP}\}\) for each \(s \in \mathcal{S}, a\in \mathcal{A}\)</li> </ul> </blockquote> <p>즉, optimal state-value function이라 함은 각 state의 state-value값을 최대화하는 policy를 적용하였을 때 가지는 값이라고 할 수 있으며, optimal action-value function 또한 마찬가지로 각 (state, action)의 action-value값을 최대화하는 policy를 적용하였을 때의 값이라고 보면 될 것이다.</p> <p>그런데, optimal state-value function의 정의가 각 state에 대한 것임에 유의하자. 다시 말해, 각 state에 대해서 state-value가 최대가 되는 policy를 선택한 것은 알겠는데, <strong>한 state의 value를 최대화시키는 \(\pi\)가 다른 state의 value도 최대화시키는가</strong>하는 문제가 남아있다. 이것이 만족되지 않으면 optimal policy라는 것은 잘 정의되었다고 말하기 어려울 것이다. 상태(state)가 바뀔 때마다 policy가 바뀌어야 한다면, 그것은 “전략”이라고 말하기가 어려울 것이기 때문이다.</p> <p><strong>따라서, 어떤 policy가 optimal이라고 하기 위해서는 다음이 만족되어야 한다.</strong></p> <blockquote> <p><strong>Optimal Policy</strong><br> A policy \(\pi^*\) of an MDP \((\mathcal{S, A, P}, R, \gamma)\) is said to be optimal if</p> <ul> <li>\(v^{\pi^*}(s) = v^*(s)\) for all \(s \in \mathcal{S}\)</li> </ul> </blockquote> <p>즉 \(\pi^*\)라는 policy를 선택하면, 모든 state의 state-value값이 최대가 되는 것이다. 이런 \(\pi^*\)가 정말 존재하기는 할까? 놀랍게도 항상 존재한다는 것을 증명할 수 있다.</p> <blockquote> <p><strong>Theorem</strong><br> For every MDP,</p> <ul> <li>there exists an optimal policy</li> <li>every optimal policy \(\pi^*\) achieves the optimal action-value function <ul> <li>i.e. \(q^{\pi^*}(s, a) = q^*(s, a)\) for all \(s\in \mathcal{S}, a\in \mathcal{A}\)</li> </ul> </li> <li>\(v^*(s) = \text{max}_{a\in \mathcal{A}} q^*(s, a)\).</li> </ul> </blockquote> <p>풀어서 설명하자면, 모든 Markov decision process에는 optimal policy가 존재할 뿐만 아니라 이 policy를 적용하면 모든 action의 action-value function이 최대화된다. 게다가 이게 끝이 아니다.</p> <blockquote> <p><strong>Theorem</strong><br> There exists a <strong>deterministic</strong> optimal policy for any MDP</p> </blockquote> <p>이러한 optimal policy중에서는 deterministic한 것만 고려해주어도 된다. 즉, 각 state에서 어떤 action을 취할지 결정하는 \(\pi^*\)는 확률분포가 아닌, 결정론적인 형태가 되어도 state-value들을 최대화시킨다는 것이다. 따라서 앞으로는 deterministic한, \(\pi: \mathcal{S}\to \mathcal{A}\)의 함수 형태로서의 policy만을 고려하겠다.</p> <p>이제 만약 optimal action-value function \(q^*\)를 찾았다고 한다면 \(\pi^*\)는 쉽게 구할 수 있다. action-value function이 최대가 되게 하는 action을 선택해주기만 하면 그게 바로 최선의 전략이 되기 때문이다. 식으로 표현해주면 다음과 같다.</p> \[\pi^*(s) = a \Leftrightarrow q^*(s, a) = \text{max}_{a'\in \mathcal{A}} q^*(s, a')\] <h2 id="bellman-optimality-equations">Bellman Optimality Equations</h2> <p>그렇다면 이제 optimal action-value function \(q^*\)를 구하는 과정만이 남았다. 이를 위해 먼저 <strong>Bellman Optimality Equation</strong>이라는 방정식을 한번 살펴보자. Dynamic Programming을 개발하고 Bellman-Ford 알고리즘을 제안한 그 벨만이다.</p> <blockquote> <p><strong>Bellman Optimality Equations</strong></p> \[v^*(s) = \text{max}_{a\in \mathcal{A}} \left( R(s, a) + \gamma \sum_{s'\in \mathcal{S}}\mathcal{P}(s, a, s')\cdot v^*(s') \right)\] \[q^*(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot\left(\max _{a^{\prime} \in \mathcal{A}} q^*\left(s^{\prime}, a^{\prime}\right)\right)\] </blockquote> <p>우선 위의 식은 아래의 식에 \(v^*(s) = \max_{a\in \mathcal{A}} q^*(s, a)\)라는 조건을 적용해주면 바로 나오는 것을 알 수 있다. \(q^*\)에 대한 아래의 식만 살펴보자. 이 또한 \(v^*(s) = \max_{a\in \mathcal{A}} q^*(s, a)\)이므로</p> \[q^*(s, a) = R(s, a) + \gamma \sum_{s'\in \mathcal{S}} \mathcal{P}(s, a, s') \cdot v^*(s)\] <p>과 동치이다. 이렇게 생각하면 어렵지 않게 받아들일 수 있는 식이 된다. (state, action)의 value는 그 reward에다가, 현재의 state에서 transition하는 다음 state의 state-value의 기댓값에 \(\gamma\)의 discount factor를 곱한 것이다.</p> <p>Bellman Optimality Equation은 좌변과 우변에 모두 각각 \(v^*\)와 \(q^*\)가 나온다. 따라서 closed-form으로 해를 정확하게 구하는 것은 불가능하며, iterative method들을 사용하여 해를 구할 수 있다. 이를 구하는 방법으로는 다음이 있다.</p> <ul> <li>Dynamic Programming: policy iteration, value iteration</li> <li>Monte-Carlo Learning</li> <li>Temporal-difference control: SARSA, Q-learning</li> </ul> <p>각각에 대해서는 다음 포스팅에서 계속해서 살펴보겠다.</p> <h1 id="참고문헌">참고문헌</h1> <p>Lecture 06: Reinforcement Learning, CS Semiar (Machine Learning in Practice) lecture slides, Korea Science Academy of KAIST</p> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Seojune Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>